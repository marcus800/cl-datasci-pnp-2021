{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Session 6: Visualisation and embeddings\n",
    "\n",
    "*Notebook by Damon Wischik*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.linear_model\n",
    "import scipy.optimize\n",
    "import sklearn.decomposition\n",
    "import sklearn.manifold\n",
    "import sklearn.model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to use and evaluate dimension reduction techniques (PCA and t-SNE) on the California housing dataset. The assignment is at the end of this notebook, in Section 2. First, here is some code\n",
    "that illustrates PCA and t-SNE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Demo code\n",
    "## 1.1 Import and plot a dataset\n",
    "\n",
    "We'll use the same dataset as in lecture 1, the World Bank dataset of statistics about states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/ekochmar/cl-datasci-pnp/master/lecture1-introduction/data/country-stats.csv'\n",
    "countries = pandas.read_csv(url)\n",
    "countries.iloc[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a splom. There's lots of messy matplotlib code to make plots look nice, and it calls for frequent use of stackoverflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick out the first three columns, excluding \"Country Name\", to show in the splom\n",
    "features = countries.columns[1:4]\n",
    "\n",
    "with plt.rc_context({'figure.figsize': (8,8)}):\n",
    "    fig,ax = plt.subplots(len(features), len(features), sharex='col', sharey='row')\n",
    "\n",
    "# We'll plot histograms on the diagonal, so they shouldn't share y-axis with the scatter plots\n",
    "for i in range(len(features)):\n",
    "    ax[i,i].get_shared_y_axes().remove(ax[i,i])\n",
    "\n",
    "# Plot histograms or scatter plots as appropriate\n",
    "for i,c in enumerate(features):\n",
    "    for j,d in enumerate(features):\n",
    "        if i == j:\n",
    "            ax[i,j].hist(countries[d], bins=30)\n",
    "        else:\n",
    "            ax[i,j].scatter(countries[d], countries[c], alpha=.2)\n",
    "\n",
    "# Rotate tick labels to make them legible\n",
    "for i,c in enumerate(features):\n",
    "    ax[i,0].set_ylabel(c, rotation=0, horizontalalignment='right')\n",
    "for j,d in enumerate(features):\n",
    "    ax[len(features)-1,j].set_xlabel(d, rotation=-30, ha='left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 One-dimensional PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an entirely manual version of one-dimensional PCA. You never need to code PCA yourself, since there are much more efficient libraries to do it for you, but it's useful to see how little there is to it. If you want to build fancier algorithms for dimension reduction, such as autoencoders, you'll have to code similar steps to these, perhaps implementing them in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx,fy = 'Enrolment Rate, Tertiary (%)', 'Internet Users (%)'\n",
    "\n",
    "# We'll look for a straight line fit   y = m*x + c.\n",
    "# This isn't the best parameterization (it can't represent a vertical line), but it's simple\n",
    "# and sufficient for this problem.\n",
    "\n",
    "def proj(x, y, θ):\n",
    "    # Given vectors x and y, project them onto the line θ=(m,c)\n",
    "    m,c = θ\n",
    "    y0,dx,dy = c,1,m\n",
    "    d = np.sqrt(dx**2 + dy**2)\n",
    "    dx,dy = dx/d, dy/d\n",
    "    dot = x*dx + (y-y0)*dy\n",
    "    return dot*dx, y0 + dot*dy\n",
    "\n",
    "def err(θ):\n",
    "    # Given straight-line parameters θ=(m,c), find the mean square error of all the projections\n",
    "    px,py = proj(countries[fx], countries[fy], θ)\n",
    "    ex,ey = countries[fx]-px, countries[fy]-py\n",
    "    return np.mean(ex**2 + ey**2)\n",
    "\n",
    "# Run an optimizer to find the error-minimizing parameters.\n",
    "# Here I'm using the built in scipy.optimize.fmin.\n",
    "# You could also implement this with gradient descent.\n",
    "\n",
    "m,c = scipy.optimize.fmin(err, x0=[.9,10])\n",
    "\n",
    "# Project the data onto the fitted straight line\n",
    "predx,predy = proj(countries[fx], countries[fy], (m,c))\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(4,4))\n",
    "ax.scatter(countries[fx], countries[fy], alpha=.3)\n",
    "for x,y,px,py in zip(countries[fx], countries[fy], predx, predy):\n",
    "    ax.arrow(x,y, px-x,py-y, color='orange', zorder=-1)\n",
    "ax.set_xlabel(fx)\n",
    "ax.set_ylabel(fy)\n",
    "plt.title(f\"y = {c:.5} + x*{m:.5}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the same model, fitted using the PCA library function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx,fy = 'Enrolment Rate, Tertiary (%)', 'Internet Users (%)'\n",
    "X = countries[[fx,fy]].values\n",
    "\n",
    "pca = sklearn.decomposition.PCA()\n",
    "pca_result = pca.fit_transform(X)\n",
    "\n",
    "# Find the predictions from the PCA model, using just the first component\n",
    "μx,μy = pca.mean_\n",
    "δx,δy = pca.components_[0]\n",
    "λ = pca_result[:,0]\n",
    "predx,predy = μx+λ*δx, μy+λ*δy\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(4,4))\n",
    "ax.scatter(X[:,0], X[:,1], alpha=.3)\n",
    "for x,y,px,py in zip(X[:,0], X[:,1], predx, predy):\n",
    "    ax.arrow(x,y, (px-x),py-y, color='orange', zorder=-1)\n",
    "ax.set_xlabel(fx)\n",
    "ax.set_ylabel(fy)\n",
    "plt.title(f\"y-{μy:.3} = {δy/δx:.5} * (x-{μx:.3})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Full PCA\n",
    "\n",
    "Here is the full PCA, run on the entire set of features. \n",
    "\n",
    "PCA looks for a good approximation to your dataset, and it treats an error of size $e$ in one feature as just as serious as an error of size $e$ in any other feature. It's therefore wise to scale the features appropriately. If you don't have any real insight into how you should weight them, then just make all your columns have the same variance.\n",
    "\n",
    "The standard library call fits the model\n",
    "$$\n",
    "\\vec{x}_i = \\vec{\\mu} + \\sum_{k=1}^K \\lambda_{k,i} \\vec{\\delta}_k\n",
    "$$\n",
    "where $K$ is the total number of features in the dataset, and $\\vec{x}_i$, $\\vec{\\mu}$, $\\vec{\\delta}_k$ are all $K$-dimensional vectors. The basis vectors it returns, $\\vec{\\delta}_1,\\dots,\\vec{\\delta}_K$, are orthonormal.\n",
    "\n",
    "PCA puts the components $\\vec{\\delta}_k$ so that the most important comes first, i.e. so that if you want to approximate the data using only $L<K$ components then you should use the first $L$,\n",
    "$$\n",
    "\\vec{x}_i = \\vec{\\mu} + \\sum_{k=1}^L \\lambda_{k,i} \\vec{\\delta}_k + \\vec{\\varepsilon}_i\n",
    "$$\n",
    "where the error term $\\vec{\\varepsilon}_i$ the sum of terms from components $\\{L+1,\\dots,K\\}$. Since the basis vectors\n",
    "are orthogonal, $\\vec{\\varepsilon}_i$ is orthogonal to $\\{\\vec{\\delta}_1,\\dots,\\vec{\\delta}_L\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the column 'Country Name', which isn't numeric\n",
    "features = countries.columns[1:]\n",
    "X = countries[features].values\n",
    "\n",
    "# rescale the features, so they have the same variance\n",
    "for k in range(len(features)):\n",
    "    X[:,k] = X[:,k] / np.std(X[:,k])\n",
    "\n",
    "pca = sklearn.decomposition.PCA()\n",
    "pca_result = pca.fit_transform(X)\n",
    "\n",
    "# how the results are returned\n",
    "print(\"μ:\", pca.mean_)\n",
    "k,i = 2,10\n",
    "print(f\"δ[k={k}]:\", pca.components_[k])\n",
    "print(f\"λ[k={k},i={i}]:\", pca_result[i,k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA gives us a nice way to summarize a large number of features in two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1,p2 = pca_result[:,0], pca_result[:,1]\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(14,11))\n",
    "ax.scatter(p1, p2, alpha=.2)\n",
    "for x,y,s in zip(p1,p2,countries['Country Name']):\n",
    "    ax.text(x,y,s)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 t-SNE\n",
    "\n",
    "t-SNE is another tool for dimension reduction. You have to tell it how many components you want it to produce (unlike PCA, which produces all components and then it's up to you how many to use)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use all the numerical features, and drop the column 'Country Name'\n",
    "features = countries.columns[1:]\n",
    "\n",
    "# as with PCA, we should be careful about the scale of data we present to the algorithm\n",
    "X = countries[features].values\n",
    "for k in range(len(features)):\n",
    "    X[:,k] = X[:,k] / np.std(X[:,k])\n",
    "    \n",
    "tsne = sklearn.manifold.TSNE(n_components=2, verbose=1)\n",
    "tsne_results = tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1,p2 = tsne_results[:,0], tsne_results[:,1]\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(14,14))\n",
    "ax.scatter(p1, p2, alpha=.2)\n",
    "for x,y,s in zip(p1,p2,countries['Country Name']):\n",
    "    plt.text(x,y,s)\n",
    "# Force the plot to have equal aspect ratio, i.e. one unit on the x-axis = one unit on the y-axis\n",
    "ax.set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 15em\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/ekochmar/cl-datasci-pnp/master/DSPNP_practical1/housing/housing.csv'\n",
    "housing = pandas.read_csv(url)\n",
    "features = ['housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income', 'median_house_value']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.** Use principal components analysis (PCA) on the above list of features. You should scale the columns appropriately. Plot a scatter plot of the first two components, and colour-code the points by `ocean_proximity`.\n",
    "\n",
    "Note: A quick and dirty way to colour-code a scatter plot in `matplotlib` is with\n",
    "```\n",
    "for lvl in distinct levels of some feature z:\n",
    "    i = (z == lvl)\n",
    "    plt.scatter(x[i], y[i], label=lvl)\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1.1, 1))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.** Answer question 1 but using t-SNE rather than PCA. Which method seems to produce more helpful clustering?\n",
    "\n",
    "Note: For speed, you may like to answer questions 1 and 2 working with a subset of several thousand rows, rather than using the full dataset, because t-SNE is slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.** In the t-SNE plot in Section 1.4 above, why did I use `set_aspect('equal')`? Why didn't I choose to use it for the PCA plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.** Repeat question 2 but using a range of values of perplexity, in the range \\[5,50\\]. Do you see different clustering for different values of perplexity?\n",
    "\n",
    "Note: perplexity is mentioned in the [sklearn.manifold.TSNE reference](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html). Here's a good read on [how to use t-SNE effectively](https://distill.pub/2016/misread-tsne/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.** Run PCA as in question 1. Let $X$ be the full feature matrix, of size $n\\times K$, and let $\\tilde{X}^L$ be the approximation to $X$ using only the first $L$ principal components, $L=0,\\dots,K$. Plot a bar chart with $L$ on the $x$-axis, and the mean square error \n",
    "$$\n",
    "\\operatorname{MSE} = \\frac{1}{n}\\sum_{i=1}^n \\sum_{k=1}^K(X_{i,k}-\\tilde{X}^L_{i,k})^2\n",
    "$$ on the $y$-axis.\n",
    "\n",
    "Hint: [`plt.bar(x,y)`](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.bar.html) plots a bar char."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6 (optional).** We'd like to decide how many components to use with PCA.\n",
    "The number of components to use, call it $L$, is a hyperparameter, and the usual way to choose a hyperparameter is by cross validation.\n",
    "But it takes some thinking to find a good way to run cross validation on an autoencoder...\n",
    "Suppose we've implemented PCA as a pair of functions\n",
    "* `enc(X,L)` which encodes a $n\\times K$ dataset $X$ into a $n\\times L$ matrix, keeping the first $L$ components for each record\n",
    "* `dec(Λ)` which decodes a $n\\times L$ set of components into a $n\\times K$ reconstruction of the data\n",
    "\n",
    "The naive approach to cross validation would be to train `enc,dec` on a training set of data, to set aside a test set `Xtest`, and to pick $L$ to make\n",
    "the reconstruction `dec(enc(Xtest,L))` as close as possible to the original `Xtest`. But this is useless, because $L=K$ will always yield perfect reconstruction.\n",
    "\n",
    "An alternative approach is to measure *denoising ability*. If we see this picture \n",
    "\n",
    "<div>\n",
    "<img src=\"weasey6.jpg\" style=\"height:8em\"/>\n",
    "</div>\n",
    "\n",
    "we don't think \"This is a weasel with no back legs\", we think \"This is a normal weasel and its back legs are occluded\". In other words, from the occluded image, we can find the low-dimensional latent representation \"this is a weasel\", from which we can predict how the animal would look if it weren't occluded. This is called *denoising*.\n",
    "\n",
    "Likewise, we can measure the performance of an autoencoder by the difference between `Xtest` and `dec(enc(n(Xtest),L))` where `n` is some function that adds noise.\n",
    "\n",
    "Your task is to use cross-validation to pick the optimal number of components $L$ to use, as measured by denoising ability for the two types of noise below. Here is a code skeleton to\n",
    "get you started.\n",
    "\n",
    "```\n",
    "def noisify_gaussian(x): \n",
    "    σ = np.std(x, axis=0)\n",
    "    return np.random.normal(loc=x, scale=σ)\n",
    "\n",
    "def noisify_mask(x):\n",
    "    μ = np.mean(x, axis=0)\n",
    "    n,K = x.shape\n",
    "    k = np.random.choice(np.arange(K), size=n)\n",
    "    y = np.copy(x)\n",
    "    y[np.arange(n), k] = μ[k]\n",
    "    return y\n",
    "\n",
    "# A more general form of noise, parameterized by the scale\n",
    "def noisify_gaussiank(k=1):\n",
    "    def n(x):\n",
    "        σ = np.std(x, axis=0)\n",
    "        return np.random.normal(loc=x, scale=σ*k)\n",
    "    return n\n",
    "    \n",
    "    \n",
    "def train_model(X):\n",
    "    ...\n",
    "    def enc(Xnew, L):\n",
    "        return ...\n",
    "    def dec(Λ):\n",
    "        return ...\n",
    "    return enc,dec    \n",
    "\n",
    "# Sanity check, on a single train/test split\n",
    "\n",
    "df = housing.dropna(subset=features)\n",
    "X = df[features].values\n",
    "n,K = X.shape\n",
    "\n",
    "kf = sklearn.model_selection.KFold(n_splits=10)\n",
    "i_train, i_test = next(kf.split(np.arange(n)))\n",
    "Xtrain, Xtest = X[i_train], X[i_test]\n",
    "enc,dec = train_model(Xtrain)\n",
    "\n",
    "# We should get perfect reconstruction, if we use all K components\n",
    "assert np.allclose(dec(enc(Xtest,K)), Xtest)\n",
    "# Imperfect reconstruction, if we use fewer\n",
    "assert not np.allclose(dec(enc(Xtest,K-1)), Xtest)\n",
    "\n",
    "# Denoising, using 3 components\n",
    "dec(enc(noisify_mask(Xtest),3))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7 (optional).** Use PCA on the expanded set of features consisting of `features` as specified in the assignment, plus `ocean_proximity` encoded numerically. \n",
    "Explain how you have chosen to encode it, and how you will scale the columns. Apply PCA, and plot the first two components. Is the output more useful than that of question 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
